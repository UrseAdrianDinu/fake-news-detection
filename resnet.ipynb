{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_categories = {\n",
    "    0: \"știri_fabricate\",\n",
    "    1: \"știri_ficționale\",\n",
    "    2: \"știri_plauzibile\",\n",
    "    3: \"știri_propagandistice\",\n",
    "    4: \"știri_reale\",\n",
    "    5: \"știri_satirice\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        model = models.resnet152(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "        pool_func = (\n",
    "            nn.AdaptiveAvgPool2d\n",
    "            if args.img_embed_pool_type == \"avg\"\n",
    "            else nn.AdaptiveMaxPool2d\n",
    "        )\n",
    "\n",
    "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
    "            self.pool = pool_func((args.num_image_embeds, 1))\n",
    "        elif args.num_image_embeds == 4:\n",
    "            self.pool = pool_func((2, 2))\n",
    "        elif args.num_image_embeds == 6:\n",
    "            self.pool = pool_func((3, 2))\n",
    "        elif args.num_image_embeds == 8:\n",
    "            self.pool = pool_func((4, 2))\n",
    "        elif args.num_image_embeds == 9:\n",
    "            self.pool = pool_func((3, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
    "        out = self.pool(self.model(x))\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out  # BxNx2048\n",
    "\n",
    "\n",
    "class ImageClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.img_encoder = ImageEncoder(args)\n",
    "        self.clf = nn.Linear(args.img_hidden_sz * args.num_image_embeds, args.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.img_encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        out = self.clf(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path, label = self.file_paths[idx], self.labels[idx]\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            if image is None:\n",
    "                print(img_path)\n",
    "                raise ValueError(f\"Error loading image at index {idx}: Image is empty or cannot be loaded\")\n",
    "\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(self.file_paths[idx])\n",
    "            print(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bebed\\anaconda3\\envs\\bigdataenv\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bebed\\AppData\\Local\\Temp\\ipykernel_9144\\3381254834.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.tensor(next(iter(train_loader))[0]).shape)\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'C:\\Users\\bebed\\OneDrive\\Desktop\\Dizertatie\\dataset\\images'\n",
    "classes = sorted(os.listdir(root_dir))\n",
    "file_paths = []\n",
    "labels = []\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(root_dir, class_name)\n",
    "    filenames = os.listdir(class_path)\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(class_path, filename)\n",
    "            file_paths.append(img_path)\n",
    "            labels.append(i)\n",
    "        \n",
    "\n",
    "\n",
    "train_file_paths, test_file_paths, train_labels, test_labels = train_test_split(\n",
    "    file_paths, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "DATA_MEAN = (0.5, 0.5, 0.5)\t\t# define the mean for the scaling transform - PIL images already come given in\n",
    "DATA_STD = (0.5, 0.5, 0.5)\t\t# define the standard deviation for the scaling transform\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "\n",
    "train_dataset = CustomDataset(train_file_paths, train_labels, transform=train_transform)\n",
    "test_dataset = CustomDataset(test_file_paths, test_labels, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(torch.tensor(next(iter(train_loader))[0]).shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    num_params = 0\n",
    "    for params in model.parameters():\n",
    "        num_params += params.shape.numel()\n",
    "\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters of models\n",
      "<class 'torchvision.models.resnet.ResNet'> :  25557032\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of parameters of models\")\n",
    "print(str(resnet_model.__class__), \": \", get_num_params(resnet_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 838 samples\n",
      "5\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 838\n",
      "    Root location: C:\\Users\\bebed\\OneDrive\\Desktop\\Dizertatie\\dataset\\images\n"
     ]
    }
   ],
   "source": [
    "data_dir = r'C:\\Users\\bebed\\OneDrive\\Desktop\\Dizertatie\\dataset\\images'\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Data augmentation transforms for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),\n",
    "    transforms.RandomApply([transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))], p=0.5),  # Add random erasing\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "# Create a dataset using ImageFolder\n",
    "full_dataset = ImageFolder(root=data_dir)\n",
    "\n",
    "full_data_loader =  DataLoader(full_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "test_size = len(full_dataset) - train_size  # 20% for testing\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Size of the dataset: {len(full_dataset)} samples\")\n",
    "\n",
    "num_classes = len(full_dataset.classes)\n",
    "print(num_classes)\n",
    "print(full_dataset)\n",
    "\n",
    "# Apply the respective transforms to each dataset\n",
    "train_dataset.dataset.transform = train_transform\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "cuda:0\n",
      "Epoch 1/50, Loss: 1.595841646194458\n",
      "Epoch 1/50, Train Loss: 1.5958, Validation Loss: 1.5797, Validation Accuracy: 0.3452\n",
      "Epoch 2/50, Loss: 1.2877551317214966\n",
      "Epoch 2/50, Train Loss: 1.2878, Validation Loss: 1.3959, Validation Accuracy: 0.4107\n",
      "Epoch 3/50, Loss: 1.4104557037353516\n",
      "Epoch 3/50, Train Loss: 1.4105, Validation Loss: 1.6400, Validation Accuracy: 0.3988\n",
      "Epoch 4/50, Loss: 1.572829008102417\n",
      "Epoch 4/50, Train Loss: 1.5728, Validation Loss: 1.5070, Validation Accuracy: 0.3750\n",
      "Epoch 5/50, Loss: 1.3616150617599487\n",
      "Epoch 5/50, Train Loss: 1.3616, Validation Loss: 1.6145, Validation Accuracy: 0.3750\n",
      "Epoch 6/50, Loss: 1.5737003087997437\n",
      "Epoch 6/50, Train Loss: 1.5737, Validation Loss: 1.5574, Validation Accuracy: 0.3929\n",
      "Epoch 7/50, Loss: 1.3507859706878662\n",
      "Epoch 7/50, Train Loss: 1.3508, Validation Loss: 1.6782, Validation Accuracy: 0.4048\n",
      "Epoch 8/50, Loss: 1.4684399366378784\n",
      "Epoch 8/50, Train Loss: 1.4684, Validation Loss: 1.4321, Validation Accuracy: 0.4167\n",
      "Epoch 9/50, Loss: 1.3496145009994507\n",
      "Epoch 9/50, Train Loss: 1.3496, Validation Loss: 1.4014, Validation Accuracy: 0.4226\n",
      "Epoch 10/50, Loss: 1.486527681350708\n",
      "Epoch 10/50, Train Loss: 1.4865, Validation Loss: 1.3620, Validation Accuracy: 0.4345\n",
      "Epoch 11/50, Loss: 1.2004413604736328\n",
      "Epoch 11/50, Train Loss: 1.2004, Validation Loss: 1.3506, Validation Accuracy: 0.4464\n",
      "Epoch 12/50, Loss: 1.17486572265625\n",
      "Epoch 12/50, Train Loss: 1.1749, Validation Loss: 1.3454, Validation Accuracy: 0.4762\n",
      "Epoch 13/50, Loss: 1.382994294166565\n",
      "Epoch 13/50, Train Loss: 1.3830, Validation Loss: 1.3465, Validation Accuracy: 0.4702\n",
      "Epoch 14/50, Loss: 1.2910842895507812\n",
      "Epoch 14/50, Train Loss: 1.2911, Validation Loss: 1.3213, Validation Accuracy: 0.4881\n",
      "Epoch 15/50, Loss: 1.4274808168411255\n",
      "Epoch 15/50, Train Loss: 1.4275, Validation Loss: 1.3283, Validation Accuracy: 0.4702\n",
      "Epoch 16/50, Loss: 1.0750216245651245\n",
      "Epoch 16/50, Train Loss: 1.0750, Validation Loss: 1.3318, Validation Accuracy: 0.4762\n",
      "Epoch 17/50, Loss: 1.3479106426239014\n",
      "Epoch 17/50, Train Loss: 1.3479, Validation Loss: 1.3281, Validation Accuracy: 0.4702\n",
      "Epoch 18/50, Loss: 1.5587942600250244\n",
      "Epoch 18/50, Train Loss: 1.5588, Validation Loss: 1.3263, Validation Accuracy: 0.4821\n",
      "Epoch 19/50, Loss: 1.6445238590240479\n",
      "Epoch 19/50, Train Loss: 1.6445, Validation Loss: 1.3339, Validation Accuracy: 0.4702\n",
      "Epoch 20/50, Loss: 1.3345685005187988\n",
      "Epoch 20/50, Train Loss: 1.3346, Validation Loss: 1.3280, Validation Accuracy: 0.4702\n",
      "Epoch 21/50, Loss: 1.3546621799468994\n",
      "Epoch 21/50, Train Loss: 1.3547, Validation Loss: 1.3301, Validation Accuracy: 0.4762\n",
      "Epoch 22/50, Loss: 1.5094889402389526\n",
      "Epoch 22/50, Train Loss: 1.5095, Validation Loss: 1.3318, Validation Accuracy: 0.4762\n",
      "Epoch 23/50, Loss: 1.2800331115722656\n",
      "Epoch 23/50, Train Loss: 1.2800, Validation Loss: 1.3307, Validation Accuracy: 0.4821\n",
      "Epoch 24/50, Loss: 1.3575690984725952\n",
      "Epoch 24/50, Train Loss: 1.3576, Validation Loss: 1.3355, Validation Accuracy: 0.4702\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "resnet_model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "# Modify the last layer for your specific number of classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(num_classes)\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(resnet_model.fc.in_features, num_classes),\n",
    ")\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_model.to(device)\n",
    "print(device)\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    resnet_model.train()  # Set the model back to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    writer.add_scalar('Train Loss', loss.item(), epoch)\n",
    "\n",
    "    resnet_model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in test_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_outputs = resnet_model(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            # Update validation loss\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Update correct predictions and total samples for accuracy calculation\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            correct_predictions += (predicted == val_labels).sum().item()\n",
    "            total_samples += val_labels.size(0)\n",
    "\n",
    "    # Calculate and log validation loss\n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "    writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "\n",
    "    # Calculate and log validation accuracy\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    writer.add_scalar('Validation Accuracy', accuracy, epoch)\n",
    "\n",
    "    # Print training and validation information\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {loss.item():.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "    early_stopping(avg_val_loss, resnet_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Save the trained model\n",
    "\n",
    "model_path = r'C:\\Users\\bebed\\OneDrive\\Desktop\\BigData\\project\\resnet152_model'\n",
    "\n",
    "torch.save(resnet_model.state_dict(), model_path)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 47.02%\n"
     ]
    }
   ],
   "source": [
    "resnet_model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        test_outputs = resnet_model(test_inputs)\n",
    "\n",
    "        # Get predicted labels\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "        # Update counts\n",
    "        total_samples += test_labels.size(0)\n",
    "        correct_predictions += (predicted == test_labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label is: 2\n"
     ]
    }
   ],
   "source": [
    "directory_path = r'C:\\Users\\bebed\\OneDrive\\Desktop\\Dizertatie\\dataset\\images\\plauzibile'\n",
    "\n",
    "# Filename\n",
    "filename = 'a1Uw9QZ5fecppuwC8i7zSjjaOLca-vaccin_sample_350.jpg'\n",
    "\n",
    "# Construct the full image path\n",
    "image_path = os.path.join(directory_path, filename)\n",
    "\n",
    "input_image = Image.open(image_path)\n",
    "input_tensor = test_transform(input_image)\n",
    "\n",
    "# Move the input tensor to the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Add batch dimension\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Move the model to the GPU\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = resnet_model(input_batch)\n",
    "\n",
    "# Interpret the model's output\n",
    "predicted_label_index = torch.argmax(predictions).item()\n",
    "# predicted_label = full_dataset.classes[predicted_label_index]\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"The predicted label is: {predicted_label_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
